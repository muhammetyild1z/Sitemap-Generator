{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6f86f60-abc2-4e4f-8035-d075fb120f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request\n",
    "from urllib.error import URLError\n",
    "from urllib.request import HTTPError\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import email.utils as eut\n",
    "from datetime import datetime\n",
    "from urllib.request import Request, urlopen\n",
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "from urllib.error import URLError, HTTPError\n",
    "from pprint import pprint\n",
    "from var_dump import var_dump\n",
    "from lxml import etree\n",
    "from lxml.html.soupparser import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c130bd40-445b-44e7-b336-8396f244bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://anifanx.com.tr/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RunCrawler(Thread-5, started 17596)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads:  1  Queue:  0  Checked:  0  Link Threads:  1\n",
      "Threads:  4  Queue:  66  Checked:  1  Link Threads:  1\n",
      "Threads:  4  Queue:  127  Checked:  5  Link Threads:  3\n",
      "Threads:  4  Queue:  160  Checked:  7  Link Threads:  3\n",
      "Threads:  4  Queue:  170  Checked:  9  Link Threads:  3\n",
      "Threads:  4  Queue:  207  Checked:  11  Link Threads:  3\n",
      "Threads:  4  Queue:  215  Checked:  13  Link Threads:  3\n",
      "Threads:  4  Queue:  233  Checked:  15  Link Threads:  3\n",
      "Threads:  4  Queue:  239  Checked:  17  Link Threads:  3\n",
      "Threads:  4  Queue:  244  Checked:  19  Link Threads:  3\n",
      "Threads:  4  Queue:  251  Checked:  21  Link Threads:  3\n",
      "Threads:  4  Queue:  255  Checked:  23  Link Threads:  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queue = []\n",
    "checked = []\n",
    "threads = []\n",
    "types = 'text/html'\n",
    "\n",
    "link_threads = []\n",
    "\n",
    "MaxThreads = 4\n",
    "\n",
    "\n",
    "InitialURL = 'https://anifanx.com.tr/'\n",
    "\n",
    "InitialURLInfo = urlparse(InitialURL)\n",
    "InitialURLLen = len(InitialURL.split('/'))\n",
    "InitialURLNetloc = InitialURLInfo.netloc\n",
    "InitialURLScheme = InitialURLInfo.scheme\n",
    "InitialURLBase = InitialURLScheme + '://' + InitialURLNetloc\n",
    "\n",
    "netloc_prefix_str = 'www.'\n",
    "netloc_prefix_len = len(netloc_prefix_str)\n",
    "\n",
    "run_ini = None\n",
    "run_end = None\n",
    "run_dif = None\n",
    "\n",
    "filename = 'sitemap.xml'\n",
    "\n",
    "request_headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "if InitialURLNetloc.startswith(netloc_prefix_str):\n",
    "    InitialURLNetloc = InitialURLNetloc[netloc_prefix_len:]\n",
    "\n",
    "class RunCrawler(threading.Thread):\n",
    "    # crawler start\n",
    "    run_ini = time.time()\n",
    "    run_end = None\n",
    "    run_dif = None\n",
    "\n",
    "    print(\"\")\n",
    "    print(InitialURL)\n",
    "    print(\"\")\n",
    "\n",
    "    if InitialURL == 'HTTPS://SOME_URL.TEST/':\n",
    "        print ('')\n",
    "        print ('Change \"InitialURL\" variable and try again!')\n",
    "        print ('')\n",
    "        sys.exit()\n",
    "\n",
    "    def __init__(self, url):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        ProcessURL(url)\n",
    "\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        run = True\n",
    "\n",
    "        while run:\n",
    "            for index, thread in enumerate(threads):\n",
    "                if thread.is_alive() == False:\n",
    "                    del threads[index]\n",
    "\n",
    "            for index, thread in enumerate(link_threads):\n",
    "                if thread.is_alive() == False:\n",
    "                    del link_threads[index]\n",
    "\n",
    "            for index, obj in enumerate(queue):\n",
    "                if len(threads) < MaxThreads:\n",
    "                    thread = Crawl(index, obj)\n",
    "                    threads.append(thread)\n",
    "\n",
    "                    del queue[index]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if len(queue) == 0 and len(threads) == 0 and len(link_threads) == 0:\n",
    "                run = False\n",
    "\n",
    "                self.done()\n",
    "            else:\n",
    "                print ('Threads: ', len(threads), ' Queue: ', len(queue), ' Checked: ', len(checked), ' Link Threads: ', len(link_threads) + 1)\n",
    "                time.sleep(1)\n",
    "\n",
    "    def done(self):\n",
    "        print ('Checked: ', len(checked))\n",
    "        print ('Running XML Generator...')\n",
    "\n",
    "        # Running sitemap-generating script\n",
    "        Sitemap()\n",
    "\n",
    "        self.run_end = time.time()\n",
    "        self.run_dif = self.run_end - self.run_ini\n",
    "\n",
    "        print(self.run_dif)\n",
    "\n",
    "\n",
    "class Sitemap:\n",
    "    urlset = None\n",
    "    encoding = 'UTF-8'\n",
    "    xmlns = 'http://www.sitemaps.org/schemas/sitemap/0.9'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root()\n",
    "        self.children()\n",
    "        self.xml()\n",
    "\n",
    "    def done(self):\n",
    "        print ('Done')\n",
    "\n",
    "    def root(self):\n",
    "        self.urlset = etree.Element('urlset')\n",
    "        self.urlset.attrib['xmlns'] = self.xmlns\n",
    "\n",
    "    def children(self):\n",
    "        for index, obj in enumerate(checked):\n",
    "            url = etree.Element('url')\n",
    "            loc = etree.Element('loc')\n",
    "            lastmod = etree.Element('lastmod')\n",
    "            changefreq = etree.Element('changefreq')\n",
    "            priority = etree.Element('priority')\n",
    "\n",
    "            loc.text = obj['url']\n",
    "            lastmod_info =  None\n",
    "            lastmod_header = None\n",
    "            date = datetime.utcnow().isoformat().replace('.', '+').split(\"+\")\n",
    "            dates=date[0]+\"+00.00\"\n",
    "            lastmod.text = dates\n",
    "            priority.text = '0.9'\n",
    "\n",
    "            if hasattr(obj['obj'], 'info'):\n",
    "                lastmod_info = obj['obj'].info()\n",
    "                lastmod_header = lastmod_info[\"Last-Modified\"]\n",
    "\n",
    "\n",
    "            # check if 'Last-Modified' header exists\n",
    "            if lastmod_header != None:\n",
    "                lastmod.text = FormatDate(lastmod_header)\n",
    "\n",
    "            if loc.text != None:\n",
    "                url.append(loc)\n",
    "\n",
    "            if lastmod.text != None:\n",
    "                url.append(lastmod)\n",
    "\n",
    "            if changefreq.text != None:\n",
    "                url.append(changefreq)\n",
    "\n",
    "            if priority.text != None:\n",
    "                url.append(priority)\n",
    "\n",
    "            self.urlset.append(url)\n",
    "\n",
    "    def xml(self):\n",
    "        f = open(filename, 'w')\n",
    "        \n",
    "        print (etree.tostring(self.urlset, pretty_print=True, encoding=\"unicode\", method=\"xml\"), file=f)\n",
    "        f.close()\n",
    "\n",
    "        print ('Sitemap saved in: ', filename)\n",
    "\n",
    "\n",
    "class Crawl(threading.Thread):\n",
    "    def __init__(self, index, obj):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.index = index\n",
    "        self.obj = obj\n",
    "\n",
    "        self.start()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        temp_status = None\n",
    "        temp_object = None\n",
    "\n",
    "        try:\n",
    "            temp_req = Request(self.obj['url'], headers=request_headers)\n",
    "            temp_res = urlopen(temp_req)\n",
    "            temp_code = temp_res.getcode()\n",
    "            temp_type = temp_res.info()[\"Content-Type\"]\n",
    "\n",
    "            temp_status = temp_res.getcode()\n",
    "            temp_object = temp_res\n",
    "\n",
    "            if temp_code == 200:\n",
    "                if types in temp_type:\n",
    "                    temp_content = temp_res.read()\n",
    "\n",
    "                    #var_dump(temp_content)\n",
    "\n",
    "                    try:\n",
    "                        temp_data = fromstring(temp_content)\n",
    "                        temp_thread = threading.Thread(target=ParseThread, args=(self.obj['url'], temp_data))\n",
    "                        link_threads.append(temp_thread)\n",
    "                        temp_thread.start()\n",
    "                    except (RuntimeError, TypeError, NameError, ValueError):\n",
    "                        print ('Content could not be parsed, perhaps it is XML? We do not support that yet.')\n",
    "                        #var_dump(temp_content)\n",
    "                        pass\n",
    "\n",
    "        except URLError as e:\n",
    "            print ('URLError: ', self.obj['url'])\n",
    "            temp_status = 000\n",
    "            pass\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print ('HTTPError: ', self.obj['url'])\n",
    "            temp_status = e.code\n",
    "            pass\n",
    "\n",
    "        self.obj['obj'] = temp_object\n",
    "        self.obj['sta'] = temp_status\n",
    "\n",
    "        ProcessChecked(self.obj)\n",
    "\n",
    "\n",
    "def dump(obj):\n",
    "    '''return a printable representation of an object for debugging'''\n",
    "    newobj=obj\n",
    "\n",
    "    if '__dict__' in dir(obj):\n",
    "      newobj=obj.__dict__\n",
    "\n",
    "      if ' object at ' in str(obj) and not newobj.has_key('__type__'):\n",
    "          newobj['__type__']=str(obj)\n",
    "\n",
    "          for attr in newobj:\n",
    "              newobj[attr]=dump(newobj[attr])\n",
    "\n",
    "    return newobj\n",
    "\n",
    "\n",
    "def FormatDate(datetime):\n",
    "    datearr = eut.parsedate(datetime)\n",
    "    date = None\n",
    "\n",
    "    try:\n",
    "        year = str(datearr[0])\n",
    "        month = str(datearr[1])\n",
    "        day = str(datearr[2])\n",
    "\n",
    "        if int(month) < 10:\n",
    "            month = '0' + month\n",
    "\n",
    "        if int(day) < 10:\n",
    "            day = '0' + day\n",
    "\n",
    "        date = year + '-' + month + '-' + day\n",
    "    except IndexError:\n",
    "        pprint(datearr)\n",
    "\n",
    "    return date\n",
    "\n",
    "\n",
    "def ParseThread(url, data):\n",
    "    temp_links = data.xpath('//a')\n",
    "\n",
    "    for temp_index, temp_link in enumerate(temp_links):\n",
    "        temp_attrs = temp_link.attrib\n",
    "\n",
    "        if 'href' in temp_attrs:\n",
    "            temp_url = temp_attrs.get('href')\n",
    "            temp_src = url\n",
    "            temp_value = temp_link.text\n",
    "            temp_url = temp_attrs.get('href')\n",
    "\n",
    "            path = JoinURL(temp_src, temp_url)\n",
    "\n",
    "            if path != False:\n",
    "                ProcessURL(path, temp_src)\n",
    "\n",
    "\n",
    "def JoinURL(src, url):\n",
    "    value = False\n",
    "\n",
    "    url_info = urlparse(url)\n",
    "    src_info = urlparse(src)\n",
    "\n",
    "    url_scheme = url_info.scheme\n",
    "    src_scheme = src_info.scheme\n",
    "\n",
    "    url_netloc = url_info.netloc\n",
    "    src_netloc = src_info.netloc\n",
    "\n",
    "    if src_netloc.startswith(netloc_prefix_str):\n",
    "        src_netloc = src_netloc[netloc_prefix_len:]\n",
    "\n",
    "    if url_netloc.startswith(netloc_prefix_str):\n",
    "        url_netloc = url_netloc[netloc_prefix_len:]\n",
    "\n",
    "    if url_netloc == '' or url_netloc == InitialURLNetloc:\n",
    "        url_path = url_info.path\n",
    "        src_path = src_info.path\n",
    "\n",
    "        if url_info.query:\n",
    "            url_path = url_path + '?' + url_info.query\n",
    "\n",
    "        src_new_path = urljoin(InitialURLBase, src_path)\n",
    "        url_new_path = urljoin(src_new_path, url_path)\n",
    "\n",
    "        path = urljoin(src_new_path, url_new_path)\n",
    "\n",
    "        #print path\n",
    "\n",
    "        value = path\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def ProcessURL(url, src = None, obj = None):\n",
    "    found = False\n",
    "\n",
    "    for value in queue:\n",
    "        if value['url'] == url:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    for value in checked:\n",
    "        if value['url'] == url:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if found == False:\n",
    "        temp = {}\n",
    "        temp['url'] = url\n",
    "        temp['src'] = src\n",
    "        temp['obj'] = obj\n",
    "        temp['sta'] = None\n",
    "\n",
    "        queue.append(temp)\n",
    "\n",
    "def ProcessChecked(obj):\n",
    "    found = False\n",
    "\n",
    "    for item in checked:\n",
    "        if item['url'] == obj['url']:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if found == False:\n",
    "        checked.append(obj)\n",
    "\n",
    "RunCrawler(InitialURL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269de6a-abfe-4bab-9e73-b81003acf38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
